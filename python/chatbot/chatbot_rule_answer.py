import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

load_dotenv()

# ë¶ˆë³€(immutable) ê·œì¹™ DBëŠ” ëª¨ë“ˆ ë¡œë“œì‹œ 1íšŒ ë¡œë“œ
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

current_dir = os.path.dirname(os.path.abspath(__file__))
db_path = os.path.join(current_dir, "rules_db_v2025")  # ìµœì‹  ê·œì¹™ ê¸°ë°˜

vector_store = Chroma(
    persist_directory=db_path,
    embedding_function=embeddings
)

retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# LLMë„ 1íšŒ ë¡œë“œ
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)


def rule_answer():
    """
    KBO ìµœì‹  ê·œì¹™(RAG) ê¸°ë°˜ ë‹µë³€ ì—”ì§„
    - ê·œì¹™ ê·¼ê±° ê¸°ë°˜
    - ì¶”ì¸¡ ê¸ˆì§€
    - íŒ¬ ì¹œí™” í‘œí˜„ (ì¡°í•­ë²ˆí˜¸ ë…¸ì¶œ X)
    """

    # ğŸ”¹ ê·œì¹™ ì§ˆë¬¸ì„ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¤ë“¬ëŠ” ë‹¨ê³„
    rephrase_prompt = ChatPromptTemplate.from_template(
    """
    ë„ˆëŠ” KBO ì•¼êµ¬ ê·œì¹™ ì „ë¬¸ê°€ì•¼.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê·œì¹™ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì •í™•í•œ ìš©ì–´ë¡œ,
    ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ½ê²Œ ì¬ì‘ì„±í•´ì¤˜.
    â–ª ì£¼ì²´(íˆ¬ìˆ˜/íƒ€ì/ì£¼ì)ê°€ ëª¨í˜¸í•˜ë©´ ëª…í™•íˆ
    â–ª í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ

    ì§ˆë¬¸: {user_question}
    ì¬êµ¬ì„±ëœ ì§ˆë¬¸:
    """
    )
    
    rephrase_chain = rephrase_prompt | llm | StrOutputParser()

    # ê·œì¹™ ê¸°ë°˜ ì •í™• ì‘ë‹µ (ì¡°í•­ë²ˆí˜¸, ì „ë¬¸ í‘œí˜„ ë“œëŸ¬ë‚´ì§€ ì•Šê¸°)
    response_prompt = ChatPromptTemplate.from_template(
    """
    ë„ˆëŠ” KBO ì•¼êµ¬ ê·œì¹™ ì „ë¬¸ AIì•¼.

    ë°˜ë“œì‹œ ì•„ë˜ ê¸°ì¤€ì„ ë”°ë¼:
    1ï¸âƒ£ ê²€ìƒ‰ëœ ê·œì¹™ ë‚´ìš©ë§Œ í™œìš©í•´ ë‹µë³€í•´.
    2ï¸âƒ£ ê·œì¹™ì— ì—†ëŠ” ê±´ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ˆ.
        - í•´ë‹¹ ê·œì¹™ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¼ê³  ë§í•´.
    3ï¸âƒ£ ì´ˆë³´ìë„ ì´í•´í•˜ê²Œ, 3ì¤„ ì´ë‚´ë¡œ.
    4ï¸âƒ£ ì¡°í•­ë²ˆí˜¸ ë“± ì „ë¬¸ì ì¸ ë¬¸êµ¬ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ˆ.
    5ï¸âƒ£ í•„ìš”í•˜ë©´ ê°„ë‹¨í•œ ì˜ˆì‹œ í•œ ì¤„ë§Œ ì¶”ê°€í•´.

    ì§ˆë¬¸: {rephrased_question}
    ê·œì¹™ ê·¼ê±°: {retrieved_context}

    ë‹µë³€:
    """
    )

    final_chain = (
        RunnablePassthrough.assign(rephrased_question=rephrase_chain)
        | RunnablePassthrough.assign(
            retrieved_context=lambda x: retriever.invoke(x["rephrased_question"])
        )
        | response_prompt
        | llm
        | StrOutputParser()
    )

    return final_chain
